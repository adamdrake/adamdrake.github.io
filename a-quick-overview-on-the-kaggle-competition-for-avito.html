<!doctype html><html lang=en><head><meta charset=utf-8><meta name=mobile-web-app-capable content="yes"><meta name=viewport content="width=device-width,initial-scale=1"><title>A Quick Overview on the Kaggle Competition for Avito - Adam Drake</title><meta name=description content="Adam Drake is an advisor to scale-up tech companies. He writes about ML/AI/data, leadership, and building tech teams."><link rel="shortcut icon" href=https://adamdrake.com/static/favicon.ico><link rel=authorization_endpoint href=https://indieauth.com/auth><link rel=token_endpoint href=https://tokens.indieauth.com/token><link rel=me href=https://github.com/adamdrake><link rel=stylesheet href=https://adamdrake.com/css/style.min.css crossorigin=anonymous media=screen><meta property="og:url" content="https://adamdrake.com/"><meta property="og:title" content="Adam Drake"><meta property="og:site_name" content="Adam Drake"><meta property="og:type" content="website"><meta property="og:description" content="Adam Drake is an advisor to scale-up tech companies. He writes about ML/AI/data, leadership, and building tech teams."><meta name=twitter:title content="Adam Drake"><meta name=twitter:description content="Adam Drake is an advisor to scale-up tech companies. He writes about ML/AI/data, leadership, and building tech teams."></head><body><header><section><div class="header flex row"><div class="header__item flex row"><a id=site__name href=https://adamdrake.com/>Adam Drake</a></div><div class="flex row"><nav aria-label="page menu" class="flex row"><ul role=menubar class="flex row"><li role=none><a class=sidebar-nav-itemmenu__item href=/ title>Latest</a></li><li role=none><a class=sidebar-nav-itemmenu__item href=/about.html title>About</a></li><li role=none><a class=sidebar-nav-itemmenu__item href=/cases.html title>Case Studies</a></li><li role=none><a class=sidebar-nav-itemmenu__item href=/contact.html title>Contact</a></li><li role=none><a class="sidebar-nav-item activemenu__item" href=/posts.html title=Posts>Posts</a></li><li role=none><a class=sidebar-nav-itemmenu__item href=/press.html title>Press</a></li><li><button class="subscribe subscribe-btn">
<a href=https://www.digitalmaneuver.com/#/portal>Subscribe to newsletter</a></button></li></ul></nav></div></div></section></header><main aria-role=main><section><ul id=feed__ul><li class="feed__li h-entry"><div class=feed__content><time class="hidden dt-published">2015-07-31 00:00:00 +0000 UTC</time><div class="flex properties__row"><div rel=author class="flex left p-author h-card"><img class=u-photo src=https://adamdrake.com/static/images/adam_drake_240.jpg alt="Adam Drake" id=author-img><div><p rel=me class=p-name id=author-name>Adam Drake</p><p class=properties>Jul 31, 2015</p></div></div><div class="flex right properties"></div></div><article class="md p-summary e-content"><h2 class=p-name>A Quick Overview on the Kaggle Competition for Avito</h2><h1 id=introduction class=anchor-link><a href=#introduction>Introduction</a></h1><p>I didn&rsquo;t have much time for this competition, so didn&rsquo;t invest much into feature engineering, creating ensembles or other things. As I participated in the <a href=https://www.kaggle.com/c/avazu-ctr-prediction>Avazu competition</a> as well, which included the use of <a href=https://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory>tinrtgu&rsquo;s now-famous code</a>, I decided to use the same approach here.</p><h1 id=background class=anchor-link><a href=#background>Background</a></h1><p>The overall goal of the competition is to analyze user behavior in order to generate a model for recommending ads to be shown in front of users, with the success metric being whether or not the user clicks on the ad. There is already a lot of work on this topic, so there is no need to rebuild everything from scratch.</p><p>If you haven&rsquo;t read the paper from Google on <a href=./static/ad-click-prediction-a-view-from-the-trenches.pdf>FTRL for ad prediction</a> and their view <em>from the trenches</em> then I can really recommend that as a first step. They deal with all the topics we find in application, like sparsity, constraints on hardware, constraints on processing times, and so on. For anyone who has also developed <a href=https://en.wikipedia.org/wiki/Real-time_bidding>real-time bidding</a> systems for <a href=https://en.wikipedia.org/wiki/Demand-side_platform>demand-side platforms</a> or <a href=https://en.wikipedia.org/wiki/Supply-side_platform>supply-side platforms</a> or other ad prediction systems this will probably be very familiar.</p><p>The data provided by Avito for the competition consisted of a sequence of views, including information on the user, ads they had seen, any possible requests to contact the seller, and so on. The data size didn&rsquo;t present any particular challenges if you use online methods as I did, as the main data file of search information is about 10GB uncompressed and contains approximately 400 million records. If you want to look further into ways to speed up the learning process, check out <a href=https://researcher.watson.ibm.com/researcher/files/us-dpwoodru/chw.pdf>Sublinear Optimization for Machine Learning</a> by Clarkson et. al.</p><h1 id=implementation class=anchor-link><a href=#implementation>Implementation</a></h1><p>The implementation was fairly quick as the solution is a known one for which Python code already exists. As is often the case, Abishek <a href=https://www.kaggle.com/c/avito-context-ad-clicks/forums/t/14516/beating-the-benchmark>posted a solution</a> to beat the benchmark. This solution took some time to run since it depends on NumPy and Pandas, which precludes my typical usage of <a href>PyPy</a> for Python code needing a bit more performance. It is easy to get around this with some decoupling, so I split the parts depending on NumPy and Pandas into a separate file thus enabling PyPy to be used for the performance-intensive code and the other code to be run separately.</p><p>The main code, which can be run by PyPy, consists of defining an ftrl object in addition to a couple of methods for fitting, predicting, and updating. Nothing crazy.</p><h1 id=building-a-test-set class=anchor-link><a href=#building-a-test-set>Building a test set</a></h1><p>If you want to assemble a test set to benchmark against locally, the following method seems to correlate very well with the score on the public leader board.</p><p>From the <a href=https://www.kaggle.com/c/avito-context-ad-clicks/forums/t/15367/proper-validation-set/86077#post86077>avito competition</a> :</p><ol><li><p>Take the minimum date that occurs in the test set (May 12?).</p></li><li><p>Take all LAST sessions for each user that happens after that date (or sample it). The last session is the one with highest date for each user.</p></li></ol><p>This problem has a time component, so sampling from other periods of time won&rsquo;t do, since it is expected that ads change over time. Randomly sampling wont do, since it will get a bigger proportion of users that has many sessions. If we take anything but the last (or n last) sessions from user we will leave the user future in the training set, so cv wont match because of overfit. Bottom line: In the data page it says that they take the last session for each user that happens after may 12. So we just need to do the same (as close as it gets)!</p><h1 id=comments-from-another-competitor class=anchor-link><a href=#comments-from-another-competitor>Comments from another competitor</a></h1><p>Andrew Ostapets, who placed 9th in competition had the following to say on the approach:</p><blockquote><p>My single ftrl model with basic features: AdID, UserID, IPID, Position, Price, â€¦ + the second- and the third-order interactions between some of this features + scores similarity between SearchQuery and Title and SearchParams and Params. It gets Public LB score below 0.044.</p><p>Adding to this model only one new feature <code>PositionFactor</code> from my teammate Alexander. It gave the incredible increments and the model scored ~ 0.0418 on Public LB.</p><p><code>PositionFactor = hash( [Position 1_place:ObjectType 2_place:ObjectType 6_place:ObjectType 7_place:ObjectType 8_place:ObjectType])</code> , where <code>Position</code> is the position of the given context ad in search result page. <code>k_place:ObjectType</code> is the position and type others ads in search result page (if there are data about them).</p><p>Alexander&rsquo;s models vw+xgb and vw+rf give him ~ 0.0424 on Public LB</p><p>After tuning hyperparameters and finding the weights for linear combination of our solutions we achieved 0.04137 on Public LB.</p></blockquote><h1 id=from-a-5th-place-finisher class=anchor-link><a href=#from-a-5th-place-finisher>From a 5th place finisher</a></h1><p>Gilberto Titericz Junior was 5th place and used this method as described by Dmitry Efimov:</p><blockquote><p>I used something similar in the Avazu contest, if you are interested, you can check my presentation <em>(link no longer available)</em> about the solution (there are some slides about Batch FTRL). The principle is to sort combined dataset (train and test) by some fields and apply the FTRL algorithm. Then it will work in the following way: the algorithm is trained on the first batch from the train set and make prediction on the first batch from the test set. Then it starts training on the second batch from the train set starting from coefficients from the first batch and so on.</p></blockquote><h1 id=conclusion class=anchor-link><a href=#conclusion>Conclusion</a></h1><p>This was an interesting and fun competition, and a good proving ground for new ideas and approaches. I wrote up an implementation in Go which I may package as a library and put on GitHub at some point. Until then, here is a forked gist from <a href=https://github.com/ceshine>ceshine</a> of the implementation from another competition:</p><a class=hidden href=https://brid.gy/publish/mastodon></a><a class=hidden href=https://brid.gy/publish/twitter></a><a class=hidden href=https://fed.brid.gy/></a><data class=p-bridgy-omit-link value=false></data></article></div><div id=webmentions></div></li></ul></section></main><hr><footer class="flex col"><section class="footer-bio content"><p><strong>Adam Drake</strong> leads technical business transformations in global and multi-cultural environments. He has a passion for helping companies become more productive by improving internal leadership capabilities, and accelerating product development through technology and data architecture guidance. Adam has served as a White House Presidential Innovation Fellow and is an IEEE Senior Member.</p></section><button class="subscribe subscribe-btn">
<a href=https://www.digitalmaneuver.com/#/portal>Subscribe to newsletter</a></button><div class=social-icons><a rel=me href=https://github.com/adamdrake title=GitHub><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></footer></body></html>